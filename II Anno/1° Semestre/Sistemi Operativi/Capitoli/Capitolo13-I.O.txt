CAPITOLO 13:  SISTEMI DI I/O 

6. INDRODUZIONE 
Il controllo dei dispositivi connessi a un calcolatore è una delle questioni più importanti che riguardano i progettisti di sistemi operativi. Poiché i dispositivi di I/O sono così largamente diversi per funzioni e velocità ad esempio un mouse, un disco e un jukebox di CD-ROM, e altrettanto diversi devono essere i metodi di controllo. Tali metodi costruiscono il sottosistema di I/O del kernel. D’altra parte, però, si assiste a una crescente varietà di dispositivi di I/O; alcuni di loro sono tanto diversi dai dispositivi precedenti dal rendere molto difficile il compito di integrarli nei calcolatori e nei sistemi operativi esistenti. 

ARCHITETTURE E DISPOSITIVI DI I/O 
I calcolatori fanno funzionare un gran numero di tipi di dispositivi: la maggior parte rientra nella categoria dei dispositivi di memorizzazione secondaria e terziaria , dispositivi di trasmissione. Un dispositivo comunica con un sistema di calcolo inviando segnali attraverso un cavo o attraverso l’etere e comunica con il calcolatore tramite un punto di connessione (porta), ad esempio una porta seriale. Se uno o più dispositivi usano in comune un insieme di fili. La connessione e detta bus. Un bus è un insieme di fili e un protocollo rigorosamente definito che specifica l’insieme dei messaggi che si possono inviare attraverso i fili. In termini elettronici, i messaggi si inviano tramite configurazioni di livelli di tensione elettrica applicate ai fili con una definita scansione temporale. I bus sono ampiamente usati nell’architettura dei calcolatori. Un calcolatore è un insieme di componenti elettronici che può far funzionare un porta, un bus o un dispositivo. Un controllore di porta seriale è un semplice controllore di dispositivo; di tratta di un singolo circuito integrato nel calcolatore che controlla i segnali presenti nei fili della porta seriale. Alcuni dispositivi sono dotati di propri controllori incorporati. L’unità d’elaborazione dà comandi e fornisce dati al controllore per portare a termine trasferimenti di I/O tramite uno o più registri per dati e segnali di controllo. La comunicazione con il controllore avviene attraverso la lettura e la scrittura, da parte dell’unità d’elaborazione, di configurazioni di bit in questi registri. Un modo in cui questa comunicazione può avvenire è tramite l’uso di speciali istruzioni di I/O che specificano il trasferimento di un byte o una parola a un indirizzo di porta I/O. l’istruzione di I/O attiva le linee di bus per selezionare il giusto dispositivo e trasferire bit dentro o fuori dal registro di dispositivo. Certi sistemi usano le tecniche. I PC ad esempio usano istruzioni di I/O per controllare alcuni dispositivi e l’I/O mappato in memoria per controllare altri. il controllore della grafica ha alcune porte di I/O per le operazioni di controllo di base, ma dispone di un’ampia regione lappata in memoria, detta memoria grafica, che serve a mantenere i contenuti dello schermo. Il processo scrive sullo schermo inserendo i dati nella regione lappata in memoria; il controllore genera l’immagine dello schermo sulla base del contenuto di questa regione di memoria. Una porta di I/O consiste in genere in quattro registri: status, control, data-in e data-out. 
 
? La CPU legge il registro data-in per ricevere dati. 
? La CPU scrive nel registro data-out per emettere dati. 
? Il registro status contiene alcuni bit che possono essere letti e indicano lo stato della porta; ad esempio indicano se è stata portata a termine l’esecuzione del comando corrente 

Il registro control può essere scritto per attivare un comando o per cambiare il modo di funzionamento del dispositivo. Ad esempio, un certo bit nel registro control della porta seriale determina il tipo di comunicazione tra half-duplex e full-duplex  La tipica dimensione dei registri di dati varia tra 1 e 4 byte. Certi controllori hanno circuiti integrati FIFO che possono contenere parecchi byte per l’immissione e l’emissione dei dati, in modo da espandere la capacità del controllore oltre la dimensione del registro di dati. 

|||||INTERROGAZIONE CICLICA
Il protocollo completo per l’interazione fra la CPU e un controllore può essere intricato, ma la fondamentale nozione di negoziazione (handshaking) è semplice, ed è illustra con un esempio. Il controllore specifica il suo stato per mezzo del bit busy del registro status; pone a 1 il bit busy quando è impegnato in un’operazione, e lo pone a 0 quando è pronto a eseguire il comando successo. La CPU comunica le sue richieste tramite il bit comand-ready nel registro command; pone questo bit a 1 quando il controllore deve eseguire un comando.   
1. La CPU legge ripetutamente il bit busy finché non valga 0. 
2. La CPU pone a 1 il bit write del registro dei comandi e scrive un byte nel registro data-out  
3. La CPU pone a 1 il bit command-ready . 
4. Quando il controllore si accorge che il bit command-ready è posto a 1, pone a 1 il bit busy 
5. Il controllore legge il registro dei comandi e trova il comando write; legge il registro data-out per ottenere il byte da scrivere, e compie l’operazione di scrittura nel dispositivo  
6. Il controllore pone a 0 il bit command-ready, pone a 0 il bit error nel registro status per indicare che l’operazione di I/O ha avuto esito positivo, e pone a 0 il bit busy per indicare che l’operazione è terminata. 

La sequenza appena descritta si ripete per ogni byte. Durante l’esecuzione del passo 1, la CPU è in attesa attiva (busy-waiting) o in interrogazione ciclica (polling): itera la lettura del registro status finché il bit busy assume il valore 0. Se il controllore e il dispositivo sono veloci, questo metodo è ragionevole, ma se l’attesa rischia di prolungarsi, sarebbe probabilmente meglio se la CPU si dedicasse a un’altra operazione. Quando, ad esempio, i dati affluiscono in una porta seriale o della tastiera. Il piccolo buffer del controllore diverrà presto pieno, e se la CPU attende troppo a lungo prima di riprendere la lettura dei byte, si prenderanno informazioni. In molte architetture di calcolatori sono sufficienti tre cicli di istruzioni di CPU per in-terrogare ciclicamente un dispositivo: read, lettura di un registro del dispositivo; logical-and, configurazione logica usata per estrarre il valore di un bit di stato, e branch, salto a un altro punto del codice se l’argomento è diverso da zero. Anziché richiede alla CPU di eseguire un’interrogazione ciclica, può essere più efficiente far si che il controllore comunichi alla CPU che il dispositivo è pronto. Il meccanismo dall’architettura che permette tale comunicazione si chiama interruzione della CPU o, più brevemente, interruzione (interrupt). 
 
|||||INTERRUZIONI
La CPU ha un contatto, detto linea di richiesta dell’interruzione, del quale la CPU controlla lo stato dopo l’esecuzione di ogni istruzione. Quando rileva il segnale di un controllore nella linea di richiesta dell’interruzione, la CPU salva lo stato corrente e salta alla routine di gestione dell’ interruzione (interrupt-handler routine), che si trova e un indirizzo prefissato di memoria. Questa procedura determina le cause dell’interruzione, porta a termine l’elaborazione necessaria ad esegue un’istruzione return from interrupt per far sì che la CPU ritorni nello stato in cui si trovava prima della sua interruzione. Il controllore del dispositivo genera un segnale d’interruzione della CPU lungo la linea di richiesta delle interruzioni, che la CPU rileva e recapita al gestore delle interruzioni, che a sua evade il compito corrispondente servendo il dispositivo. Nei sistemi operativi moderni necessarie capacità di gestione delle interruzioni più raffinate. 
 
1. Si deve poter posporre la gestione dell’interruzione durante le fasi critiche dell’elaborazione. 
2. Si deve disporre di un meccanismo efficiente per passare il controllo all’appropriato gestore delle interruzioni, senza dover esaminare ciclicamente tutti i dispositivi (polling) per determinare quale abbia generato l’interruzione. 3. Si deve disporre di più livelli d’interruzione, di modo che il sistema possa distinguere le interruzioni ad altra priorità da quelle a priorità inferiore, servendo le richieste con la celerità appropriata del caso. 
 
In un calcolatore moderno queste tre caratteristiche sono fornite dalla CPU e dal controllore delle interruzioni . 
La maggior parte delle CPU ha due  linee di richiesta delle interruzioni. Una è quella delle interruzioni non mascherabili, riservata a eventi quali gli errori di memoria irrecuperabili. La secondo linea è quella delle interruzioni mascherabili: può essere disattivata dalla CPU prima dell’esecuzione di una sequenza critica di istruzioni che non deve essere interrotta. Il meccanismo delle interruzioni accetta un indirizzo. Nella maggior parte delle architetture questo indirizzo è uno scostamento relativo a una tabella detta vettore delle interruzioni, contenente gli indirizzi di memoria gli specifici gestori delle interruzioni. In pratica, tuttavia, i calcolatori hanno più dispositivi (e quindi, più gestori delle interruzioni) che elementi nel servirsi di una tecnica detta concatenamento delle interruzioni (interrupt chaining), in cui ogni elemento del vettore delle interruzioni punta alla testa di una lista di gestori nella lista delle interruzioni. Quando si verifica un’interruzione, si chiamano uno alla volta i gestori nella lista corrispondente finché non se ne trova uno che può soddisfare la richiesta . E descritto il vettore delle interruzioni della CPU Intel Pentium. Il meccanismo delle interruzioni realizza anche un sistema di livelli di proprietà delle interruzioni. Esso permette alla CPU di differire la gestione delle interruzioni di bassa priorità senza mascherare tutte le interruzioni, e permette a un’interruzione di priorità alta di sospendere l’esecuzione della procedura di servizio di un ‘interruzione di priorità bassa. Un sistema operativo moderno interagisce con il meccanismo delle interruzioni in vari modi. All’accensione della machina esamina i bus per determinare quali dispositivi siano presenti, e installa gli indirizzi dei corrispondenti gestori delle interruzioni nel vettore delle interruzioni. Durante l’I/O, i vari controllori di dispositivi generano i segnali d’interruzione della CPU quando sono pronti per un servizio. Il meccanismo delle interruzioni si usa anche per gestire un’ampia gamma di eccezioni, come la divisione per zero, l’accesso a indirizzi di memoria protetti o inesistenti o il tentativo di eseguire un’istruzione privilegiata in modalità utente. Un altro esempio è dato dall’esecuzione delle chiamate di sistema. Solitamente i programmi sfruttano routine di libreria per eseguire chiamate di sistema. La routine controlla i parametri passati dall’applicazione, li assembla in una struttura dati appropriata da passare al kernel, e infine esegue una particolare istruzione detta interruzione software o trap. Quando la chiamata di sistema esegue l’istruzione di eccezione, l’architettura delle interruzioni memorizza le informazioni riguardanti lo stato cui era giunta l‘esecuzione del codice utente, passa al modo supervisore e recapita l’interruzione alla procedura del kernel che realizza il servizio richiesto. Le interruzioni si possono inoltre usare per gestire il controllo del flusso all’interno del kernel. Si consideri ad esempio l’elaborazione richiesta per completare una lettura da un disco. Un passo necessario è quello  i copiare dati dalla regione di memoria usata dal kernel al buffer dell’utente. Questa azione richiede tempo, ma non è urgente e non dovrebbe bloccare la gestione delle interruzioni con priorità più alta. Un altro passo è quello di avviare l’evasione desse successive richieste di IO relative a quell’unita a disco. Questo passo ha priorità più alta: se  le unta a disco si devono usare in modo efficiente, è necessario avviare l’evasione della successiva richiesta di IO non appena la precedente sia stata soddisfatta. Un’architettura del kernel basata su thread è adatta alla realizzazione di più livelli di priorità delle interruzioni e a dare la precedenza alla gestione delle interruzioni rispetta alle elaborazioni in sottofondo delle procedure del kernel e delle applicazioni. Riassumendo i segnali d’interruzione sono usati diffusamente dai SO moderni per gestire eventi asincroni e per eseguire in modalità supervisore le procedure del kernel. Per far si che i compiti più urgenti siano portati  a termine per primi, i calcolatori moderni usano un sistema di priorità delle interruzioni. I controllori dei dispositivi, gli errore i e le chiamate di sistema generano segnali d’interruzione al fine di innescare l’esecuzione di procedure del kernel.  

|||||DMA
Quando un dispositivo compie trasferimenti di grandi quantità di dati, come nel caso di un’unita a disco, l’uso di una costosa CPU per il controllo dei bit di stato e per la scrittura di dati nel registro del controllore byte alla volta, detto IO programmato, sembra essere uno spreco. In molti calcolatori si evita di sovraccaricare la CPU assegnando una parte di questi compiti a un’unita di elaborazione specializzata, della controllore dell’accesso diretto alla memoria DMA. Per dar avvio a un trasferimento DMA la CPU scrive in memoria un comando strutturato per il DMA. La CPU scrive l’indirizzo di questo comando nel controllore del DMA, e prosegue con la sua esecuzione. Il controllore DMA agisce quindi direttamente sul bus della memoria, presentano al bus gli indirizzi di memoria necessarie per eseguire il trasferimenti senza l’aiuto della CPU. Un semplice controllore DMA è un componente o ordinario dei PC e le schede di IO dette bus mastermind di un PC includono di solito componenti DMA ad alta velocità.  La procedura di negoziazione tra il controllore del DMA e il controllore del dispositivo si svolge grazie a una coppia di fili detti DMA request e DMA acknowledge. Il controllore del dispositivo manda un segnale sulla linea DMA request quando una parola di dati è disponibile per il trasferimento. Questo segnale fa si che il controllore DMA prenda possesso del bus di memoria, presenti l'indirizzo desiderato ai fili d’intirizzimento della memoria e mandi un segnale lungo la linea DMA acknowledge. Quando il controllore del dispositivo riceve questo segnale, trasferisce in memoria la parola di dati e rimuove il segnale dalla linea DMA request. Quando l’intero trasferimento termina, il controllore del DMA interrompe la CPU. Quando il controllore del DMA prende possesso del bus di memoria, la CPU è temporaneamente impossibilitata ad accedere alla memoria centrale, sebbene abbia accesso ai dati contenuti nella sua cache primaria e secondaria. Questo fenomeno noto come sottrazione di cicli, può rallentare la computazione della CPU; ciononostante l’assegnamento del lavoro di trasferimento di dati a un controllore DMA migliora e in generale le prestazioni complessive del sistema. In alcune architetture per realizzare la tecnica DMA si usano gli indirizzi della memoria fisica, mentre in altre s’impiega l’accesso diretto alla memoria virtuale, in questo caso si usano indirizzi virtuali che poi si traducono in indirizzi fisici.  
 

||||| INTERFACCIA DI I/O PER LE APPLICAZIONI 
Si spiega come un’applicazione possa aprire n file residente in un disco senza sapere di che tipo di disco si tratti e come si possano aggiungere al calcolatore nuove unita a disco e latri dispositivi senza che si debba modificare il sistema operativo.  
I metodi qui esposti coinvolgono l’astrazione, l’incapsulamento e la stratificazione dei programmi. In particolare si puo compiere un procedimento di astrazione rispetto ai dettagli delle differenze tra i dispositivi per l’IO identificandone alcuni tipi generali. A ognuno di questi tipi si accede per mezzo di un unico insieme di funzioni- un’interfaccia. Le differenze sono incapsulate in moduli del kernel detti driver dei dispositivi. Lo scopo dello strato dei driver dei dispositivi è di nascondere al sottosistema di IO del kernel le differenze tra i controllori dei dispositivi in mood simile a quello con cui le chiamate di sistema di IO incapsulando il comportamento dei dispostivi in alcun classi generiche che nascondo le differenze alle applicazioni. Sfortunatamente per i produttori di dispositivi, ogni tipo di sistema operativo ha le sue convenienze riguardanti l’interfaccia dei driver dei dispositivi.  
I dispositivi possono differire in molti aspetti: 
? Trasferimento a flusso di caratteri o a blocchi. Un dispositivo del primo tipo trasferisce dati un byte alla volta mentre uno del secondo tipo ne trasferisce un blocco alla volta.  
? Accesso sequenziale o diretto. Un dispositivo del primo tipo trasferisce dati sentendo ordine prestabilito, mentre l’utente di un dispositivo ad accesso diretto può richiedere l’accesso a una qualunque delle possibili locazioni di memorizzazione 
? Dispositivi sincroni o asincroni. Un dispositivo sincrono trasferisce dati con un tempo di risposta prevedibile, mentre un dispositivo asincrono ha tempi di risposta irregolari 
? Condivisibili o riservati. Un dispositivo condivisibile può essere usato in modo concorrente da diversi processi mentre ciò è impossibile se un dispositivo è riservato.  
? Velocità di trasferimento. Può variare da alcuni byte a alcuni gb 
? Lettura e scritture, solo lettura o solo scrittura. Per ciò che riguarda l’accesso delle applicazioni ai dispositivi, molte di queste differenze sono nascoste dal sistema operativo e i dispostivi sono raggruppati in poche classi convenzionali.  

|||||Dispositivi con trasferimento a blocchi o a caratteri 
L’interfaccia per i dispostivi a blocchi sintetizza tutti gli aspetti necessari per accedere alle unita a disco e ad altri dispositivi basati sul trasferimento di blocchi di dati. Il SO e certe applicazioni particolari come quelle per la gestione della basi di dati possono trovare più convenirne trattare questi dispositivi come una semplice sequenza lineare di blocchi. In questo caso si parla di IO a basso livello. La tastiera è un esempio di dispositivo al quale si accede tramite un’interfaccia a flusso di caratteri. 

|||||Dispositivi di rete 
Poiché i modi di intirizzimento e le prestazioni tipiche dell’IO di rete sono notevolmente differenti da quelli dell’IO dell’unita a disco, la maggior parte dei  SO fornisce un’interfaccia per l’IO di rete diversa da quelle delle normali operazioni detta socket. Una volta creata una socket si possono usare le normali operazioni di IO. 

|||||Orologi e timer 
La maggior parte dei calcolatori ha timer e orologi per l’invio di segnali, segnalare l’ora corrente e in tempo trascorso. Il dispositivo che misura la durata di un lasso di tempo e che può avviare un’operazione si chiama timer programmabile. 

|||||IO bloccante e non bloccante 
Un altro aspetto delle chiamate di sistema è la scelta fra IO bloccante e non bloccante. Quando un’applicazione impiega una chiamata di sistema bloccante si sospende l’esecuzione delle applicazioni, che passa dalla coda dei processi pronti per l’esecuzione alla coda d’attesa. Quando la chiamata di sistema termina l’applicazione è posta nuovamente nella coda dei processi pronti in modo che possa riprendere l’esecuzione solo allora essa ricevere i valori riportati dalla chiamata di sistema.  Alcuni processi a livello utente necessitano di una forma di IO non bloccante. Un esempio è quello di un’interfaccia utente con cui s’interagisce col mouse e la tastiera mentre elabora dati e li mostra sullo schermo.  La differenza tra chiamata non bloccante e asincrona è che una read non bloccante restituisce immediatamente il controllo fornendo i dati che è stato possibile leggere. Una chiamata read asincrona richiede un trasferimento di cui il sistema garantisce il completamento ma solo in un momento successivo e non prevedibile.  
 

|||||  SOTTOSISTEMA PER L’I/O DEL KERNEL 
Il kernel fornisce molti servizi riguardanti l’IO e scheduling dell’IO gestione del buffer, della cache.   

|||||Scheduling di IO 
Fare lo scheduling di un insieme di richieste di IO significa stabilirne un ordine d’esecuzione efficace. Lo scheduling può migliorare le prestazioni complessive del sistema , distribuire equamente gli accessi dei processi ai dispositivi dei processi ai dispositivi e ridurre il tempo d’attesa media per il completamento di un’operazione di IO. I progettisti di SO realizzano scheduling mantenendo una coda di richieste per ogni dispositivo. Quando un’applicazione richiede l’esecuzione di una chiamata di sistema di IO bloccante, si aggiunge la richiesta alla coda relativa al dispositivo appropriato. Lo scheduler dell’IO riorganizza l’ordine della coda per migliorare l’efficienza totale del sistema e il tempo medio d’attesa cui sono sottoposte le applicazioni. Il sistema operativo può anche tentare di essere equo in modo che nessuna applicazione ricerca un sevizio carente o può dare fiorita a quello richieste la cui corretta esecuzione potrebbe essere inficiata in un ritardo del servizio. I kernel che mettono a disposizione di IO asincrono devono essere in grado di tener traccia di più richieste di IO contemporaneamente. A questo fine alcuni sistemi annetto una tabella dello stato dei dispostivi alla coda dei processi in attesa. Gli elementi della tabella indicano il dispositivo e lo stato.   

|||||Gestione del buffer 
Un buffer è un’area di memoria che contiene dati durante il trasferimento fra due dispositivi o tra un’applicazione e un dispositivo. Si ricorre al buffer di 3 ragioni: la prima è la necessita di gestire la differenza di velocità tra produttore e consumatore. Un modem ad esempio che è nettamente più lento di qualsiasi altro dispositivo ha bisogno di una doppia bufferizzazione altrimenti si renderebbe troppo critico il problema della sincronizzazione con il disco. Un secondo uso del buffer riguarda la gestione dei dispositivi che trasferiscono dati in blocchi di dimensione diversa. Il terzo modo in cui si può impiegare un buffer è per la realizzazione della semantica delle copie nell’ambito dell’IO delle applicazioni. La semantica delle copie garantisce che la versione dei dati scritta nel disco sia conforme a quella contenuta nel buffer al momento della chiamata di sistema indipendentemente da ogni successiva modifica. 

|||||Cache 
Una cache è una regione di memoria veloce che serve per mantener copie di certi dati: l’accesso a queste copie è più rapido dell’accesso agli originali. Ad esempio le istruzioni di un processo correntemente in esecuzione sono memorizzate in un disco copiate nella memoria fisica e copiate ulteriormente nella cache primaria e secondaria della CPU. La differenza tra buffer e cache consiste nel fatto che il primo può contenere dati di cui non esiste altra copia, la seconda mantiene su un mezzo più efficiente una copia d’informazioni gia memorizzate.

|||||Code di buffer
Una coda di file da stampare è un buffer contenente dati per un dispositivo che no può accettare flussi di dati intervallati. Sebbene una stampante possa servire una sola richiesta alla volta, diverse applicazioni devono potere richieder simultaneamente la stampa di dati, senza che questi si mischino. Il SO risolve questo problema filtrando tutti i dati per la stampante: i dati da stampare provenienti da ogni singola applicazione si registrano in uno specifico file in un disco. Quando un’applicazione termina di emettere dati da stampare, si aggiungetele file nella coda di stampa, quest’ultima viene copiata sulla stampante un file per volta. In questo caso il SO fornisce un’interfaccia di controllo che permette agli utenti e agli amministratori del sistema di esaminare la coda ed eliminare elementi della coda prima che siano stampati, sospendere una stampa e cosi via.

|||||Gestione degli errori
Un SO che usi la protezione della memoria può proteggersi da molti tipo di errori dovuti ai dispositivi o alle applicazione cosicché il blocco completo del sistema non è l’ordinaria conseguenza di piccoli difetti tecnici. I SO sono spesso capaci di compensare efficacemente le conseguenza negative dovute a errori generati da cause contingenti.  Il sistema unix usa una variabile intera detta errno per codificare tutti gli errori. 

|||||Protezione dell’IO  
Gli errori sono strettamente connessi alla tematica della protezione. Un processo utente che cerchi di impartire istruzioni IO illegali può disturbare il funzionamento normale di un sistema sia che lo faccia internazionalmente sia accidentalmente. Onde evitare che gli utenti impartiscano istruzioni di IO illegali, si definiscono come privilegiate tute le istruzioni relative all’IO. Ne consegue che gli utenti no potranno impartire in via diretta alcuna istruzione, ma dovranno farlo attraverso il SO. U programma utente per eseguire IO invoca una chiamata di sistema per chiedere al  So di svolger una data operazione del suo interesse. Inoltre il sistema di protezione della memoria deve tutelare dall’accesso degli utenti tutti gli indirizzi mappati in memoria e gli indirizzi delle porte di IO. Il kernel tuttavia non può semplicemente negare qualunque tentativo di accesso da parte degli utenti: quasi tutti i videogiochi e i programmi di grafica usano l’accesso diretto al controllore della grafica mappato in memoria. 

|||||Strutture dati del kernel 
Il kernel ha bisogno di mantenere informazioni sullo stato dei componenti coinvolti nelle operazioni di IO e usa a questo fine diverse strutture dati interne. Il SO Unix, per mezzo del file system permette l’accesso a diversi oggetti: file degli utenti, dispostivi, spazio degli indirizzi. Quando il kernel ad esempio deve leggere un file utente ha bisogno di controllare la buffer cache prima di decidere l’effettiva esecuzione. Per leggere un disco tramite un’interfaccia a basso livello, il kernel deve accertarsi del fatto che la dimensione dell’insieme dei dati di cui è stato richiesto il trasferimento sia multiplo della dimensione dei settori del disco. Alcuni SO quali windows NT usano metodi orientati agli oggetti in misura più rilevante o sullo scambio di messaggi. Quando l’operazione è di emissione di dati il messaggio contiene i dati da scrivere quando è di emissione il messaggi contiene l’indirizzo del buffer che si usa per ricevere i dati.   

|||||. TRASFORMAZIONE DELLE RICHIESTE DI IO IN OPERAZIONI DEI DISPOSITIVI 
Si consideri ad esempio la lettura di un file da un’unita a disco. L’applicazione fa riferimento ai dati per mezzo del nome del file: è compito del file system fornire il modi di giungere attraverso la struttura delle directory alla regione del disco appropriata, cioè quella dove i dati del file sono fisicamente recenti. In UNIX il nome è associato a un elemento di una lista di oggetti detti numeri di inode. La prima parte del nome in DOS identifica il dispositivo e successivamente la collocazione. Unix utilizza una tabella di montaggio per associare i prefissi dei nomi di percorso ai corrispondenti a nomi di dispostivi. La seguente descrizione del tipico svolgimento di una richiesta di lettura bloccante indica che l’esecuzione di un’operazione di IO richiede una gran quanta di passi: 
a) un processo impartisce una chiamata di sistema read bloccante relativa a un descrittore di file di un file precedentemente aperto. 
b) il codice della chiamata di sistema all’interno del kernel controlla la correttezza dei parametri. Se sono presenti nel buffer cache, si passano i dati richiesi al processo chiamante e l’operazione è conclusa.  
c) altrimenti è necessario eseguire un’operazione fisica di io cosi si rimuove il processo dalla coda dei processi pronti per l’esecuzione per inserirlo nella coda d’attesa relativa al dispositivo interessato. E si effettua lo scheduling della richiesta di OI. Infine il sottosistema per l’IO invia la richiesta al driver del dispositivo, secondo il sistema operativo ciò avviene tramite la chiamata di una proceduta o per mezzo dell’emissione di un messaggio interno al kernel.  
d) il driver del dispositivo assegna un buffer nello spazio d’indirizzi del kernel che serve per ricevere i dati immessi ed esegue lo scheduling dell’IO. Infine il driver impartisce il comandi al controllore del dispositivo scrivendo nei suo registri.  
e) il controllore aziona il dispositivo per compiere il trasferimento dei dati.  f) il driver può eseguire un’interrogazione ciclica o può avere predisposto un trasferimento DMA nella memoria del kernel.  
g) tramite le interruzioni si attiva l’appropriato gestore delle interruzioni chiedono aver memorizzato i dati necessari, avverte con un segnale il driver del dispositivo e restituisce il controllo.  
h) il driver riceve il segna , individua la richiesta di IO si accerta della riuscita o del fallimento dell’operazione e segnala al sottosistema per l’IO del kernel il completamento dell’operazione 
i) il kernel trasferisce dati e/o codice di stato nello spazio degli indirizzi del processo chiamante 
j) nel momento in cui è posto nella coda dei processi pronti per l’esecuzione il processo non è più bloccato: quando lo scheduler gli assegnerà la CPU esso riprenderà l’elaborazione. L’esecuzione della chiamata è terminata.  

|||||  PRESTAZIONI 
l’IO è uno tra i principali fattori che influiscono sulle prestazioni di un sistema: richiede un notevole impiego della CPU per l’esecuzione del codice dei driver e per uno scheduling equo ed efficiente dei processi quando essi sono bloccati o riavviati. Sebbene i calcolatori moderni siano capaci di gestire migliaia di interruzioni al secondo, la gestione delle interruzioni è un processo relativamente oneroso: ciascuna di loro fa si che il sistema cambi stato, esegua il gestore delle interruzioni e infine ripristini lo stato originario. Se il numero di cicli impiegato nell’attesa attiva non è eccessivo,l’IO programmato può essere più efficiente di quello basato sulle interruzioni. Per eliminare cambi di contesto implicati dal trasferimento di ciascun carattere dal demone del kernel, gli sviluppatori di solaris hanno implementato nuovamente il demone telnet tramite thread interni al kernel. Secondo stime della sun, queste migliorie hanno portato il massimo numero di connessioni contemporanee sostenibili da un grande server da qualche centinaio a qualche miglioro. Ad esempio i concentratori di terminali convogliano il traffico di informazioni proveniente da centinaio di terminali a un’unica porta di un grande calcolatore. I canali di IO sono unita di elaborazione specializzate presenti nei mainframe e altri sistemi di altro profilo; il loro compito è sollevare la CPU di una parte del peso della gestione dell’IO. 
Per migliorare l’efficienza dell’IO si possono applicare diversi principi: 
? Ridurre il numero dei cambi di contesto 
? Ridurre il numero di copiature dei dati in memoria durante i trasferimenti fra dispositivi e applicazioni 
? Ridurre la frequenza delle interruzioni 
? Aumentare il tasso di concorrenza usando controllori DMA intelligenti o canali di IO per sollevare la CPU dalle semplice copiature di dati 
? Realizzare le primitive direttamente tramite dispositivi fisici cosi da permettere che la loro esecuzione sia simultanea alle operazioni di bus e di CPU 
? Equilibrare le prestazioni della CPU del sottosistema 
Il driver inoltre esegue raffinati algoritmi di gestione degli errori e di recupero dati e più che hanno un’incidenza notevole sulle prestazioni complessive del sistema, svolge diverse funzioni di ottimizzazione delle presentazioni dell’unita a disco. 
 
  